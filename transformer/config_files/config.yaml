data:
  max_len: 512
  batch_size: 10000
  subset: 0.025   # or null
  data_dir: "data"
  tokenized_data_dir: "data/tokenized"

de_en:
  vocab_size: 37000
  source_language: "en"
  target_language: "de"
  hf_dataset_path: "wmt14"
  dataset_name: "de-en"
  tokenizer_path: "Helsinki-NLP/opus-mt-en-de"
  processed_data_dir: "data/tokenized/de-en_2.5"


fr_en:
  vocab_size: 32000
  source_language: "en"
  target_language: "fr"
  hf_dataset_path: "wmt14"
  dataset_name: "fr-en"
  tokenizer_path: "Helsinki-NLP/opus-mt-en-fr"
  

base_model:
  max_seq_len: 512
  d_model: 512
  d_ff: 2048
  num_layers: 6
  num_heads: 8
  dropout: 0.1
  total_steps: 100000


# Optimizer
optimizer:
  lr: 1.0
  betas: (0.9, 0.98)
  eps: 1e-09

# Scheduler
scheduler: 
  warmup_steps: 4000


# Regularization
loss: 
  label_smoothing: 0.1


# Training Hyperparameters
training:
  batch_size: 32
  total_epochs: 2
  cuda: 0

# Directories
output_dir:
  base_dir: "results"
  checkpoint_dir: "checkpoints"
  tensorboard_dir: "tensorboard"
  result_dir: "results"



